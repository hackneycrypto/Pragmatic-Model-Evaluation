{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7"}}, "nbformat": 4, "nbformat_minor": 4, "cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Pragmatic Model Evaluation \n", "\n", "## Feature Selection and Engineering\n", "\n", "### Motivation\n", "\n", "In certain contexts, it is very important to select and/or engineer useful features to feed into your ML models.\n", "\n", "If you feed your Machine Learning methods poor features from your dataset, you can expect to get poor models out.\n", "(The clich\u00e9 is \"Garbage in, Garbage out\").\n", "\n", "So, we need to identify or even create independent variables that can inform us about the dependent variable. The set of inputs *X* should predict the target variable *y*. Things that do not relate to  *y* should not be included, like noise or constants.\n", "\n", "The process of identifying which of our features we should use as input to our model is known as *feature selection*. \n", "\n", "Some datasets can have thousands, millions, or even billions of features (for example, when working with genome data). Even with smaller datasets, some ML algorithms perform poorly when correlated or noisy variables are included (e.g. linear regression), and can greatly benefit from from feature selection. More generally, models made with all available features may be over-complicated, lack generalisation and can be hard to interpret. ML models should be parsimonious, simple as possible, with low error. Otherwise, models get mis-interpreted, take too long to run, or fit the noise (over-fit)!\n", "\n", "The process of creating new features from the data is known as *feature engineering*. This can include:\n", "+ multiplying columns together in tabular data\n", "+ highlighting edges or regions of images \n", "+ PCA: rotating the axes you are using to more sensible ones and selecting information-rich features\n", "+ transforming variables by applying arbitrary functions to them\n", "+ making sure all required variables are available, not just using those readily available.\n", "\n", "Feature engineering contributes to explainability, speeds-up training, and decreases over-fitting.\n", "\n", "In fact, feature selection and engineering are often applied iteratively: \n", "Get features --> make ML model --> model optimization --> improve features --> make new model --> improve features..."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Load relevant libraries\n", "\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "import numpy as np"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Getting to know this dataset\n", "\n", "## Dataset: Boston Housing Data\n", "\n", "**Dependent Variable**: \n", "\n", "MEDV: Median value of owner-occupied homes in 1000's of dollars\n", "\n", "**Explanatory Variables**\n", "\n", "CRIM: per capita crime rate by town\n", "\n", "ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n", "\n", "INDUS: proportion of non-retail business acres per town\n", "\n", "CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n", "\n", "NOX: nitric oxides concentration (parts per 10 million)\n", "\n", "RM: average number of rooms per dwelling\n", "\n", "AGE: proportion of owner-occupied units built prior to 1940\n", "\n", "DIS: weighted distances to five Boston employment centres\n", "\n", "RAD: index of accessibility to radial highways\n", "\n", "TAX: full-value property-tax rate per 10,000 dollars\n", "\n", "PTRATIO: pupil-teacher ratio by town\n", "\n", "B: 1000(Bk - 0.63)^2 where Bk is the proportion of black residents by town\n", "\n", "LSTAT: lower status of the population "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["df = pd.read_csv(\"data/boston_housing.csv\")\n", "\n", "df.head(10)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Evaluating correlation between variables"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Q1**. Use pairplot function of `seaborn` to get a matrix of correlation plots between the variables."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Q2**. Calculate the correlation coefficient for variable AGE and the target, MEDV."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from scipy.stats import pearsonr\n", "\n", "# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Q3**. Calculate the correlation coefficient for variable RM and the target, MEDV."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Q4**.  Identify the best features using the Kendall Tau correlation coefficient. \n", "\n", "Hint: you can use corr function, where the method can be specified as 'kendall'"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Removing variables with low information content\n", "\n", "ML models should be made without using all available variables, hence we should remove some.\n", "\n", "A) A simple way to do this is to remove those that are not correlated with the target variable (E.g. MEDV as in the above example).\n", "\n", "B) Another way is to remove variables that correlate strongly with each other (remove all but one).\n", "\n", "\n", "PCA is a robust method for removing dimensions (features) that have linear correlations with each other."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Q4**. Remove the variables that have a low correlation with MEDV. Remove any variable that has correlation less than 0.03, with the target variable.\n", "\n", "For this, write a function name `keep_predictors` and take 3 args: `df`, `targ` - the target variable - and `cor_thresh` - the correlation threshold.\n", "\n", "_Hints: Import `deepcopy` from `copy` (that does not change the original data frame) and please remember to use `abs()` values._"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "# def keep_predictors(df, targ, cor_thresh):\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["targ = df[\"MEDV\"]\n", "df_redu = keep_predictors(df,targ,0.03)\n", "df_redu.head()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Q5**. Remove one of a pair of the remaining variables that have high correlations with each other.\n", "\n", "For this purpose, write a function name `remove_correlated` that takes 2 args: `df` and `cor_thresh1` - the correlation threshold.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n", "# def remove_correlated(df, cor_thresh1):\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["You may test the function, `remove_correlated` with the following code:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["cor_thresh1 = 0.8\n", "boston_minimal = remove_correlated(df, cor_thresh1)\n", "boston_minimal.head(14)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Principle Component Analysis (PCA)\n", "\n", "## PCA and z-score normalisation.\n", "\n", "PCA looks for the most variance, so can be biased based on size of values (larger values can easily have more variance). Hence data need to be mormalised before applying PCA on it. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Q6**. Drop the target variable, \"MEDV\" from the original dataset, and call it `boston_x`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Q7**. Use `StandardScaler` from `sklearn.preprocessing`, to transform the input data to z-score values."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Q8**. Now, use Principal Component Analysis (PCA) to reduce the dimensionality of this dataset to 5 Principal Components (PCs).\n", "\n", "PCA operation can be imported from the decomposition package of the sklearn.\n", "\n", "Save outputs of your PCA to the variable `pca`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Q9**. Get the explained variance ratios of these first few PCs.\n", "\n", "Hint: use the attribute called explained_variance_ratio_ \n", "\n", "Print their sum below too."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["PC_values = np.arange(pca.n_components_) + 1\n", "plt.plot(PC_values, pca.explained_variance_ratio_, 'ro-', linewidth=2)\n", "plt.title('Scree Plot')\n", "plt.xlabel('Principal Component')\n", "plt.ylabel('Proportion of Variance Explained')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["So the PCs selected don't explain all the variance, but should explain most of it.\n", "\n", "The Pareto Principle says that people can get most of the work done by focusing on a small percentage of the work.\n", "\n", "This is also called the 80-20 Rule: For example: you can get 80% of success by focusing on only 20% of the tasks.\n", "\n", "OR an even better possibility, 90% of success by focusing on only 15% of the tasks. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Bonus**\n", "\n", "If you would like to know what went into making the PC, the following exercise will focus on that. \n", "\n", "What factors have the most variance in this dataset?\n", "(Could be related to predicting MEDV, the median values of houses.)\n", "\n", "Giving you the answer here, see seralouk's answer:\n", "\n", "https://stackoverflow.com/questions/22984335/recovering-features-names-of-explained-variance-ratio-in-pca-with-sklearn"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Q10 bonus**. Extract the features present from the PCs.\n", "\n", "_Hint: `pca.components_`_ may be helpful"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Some of these features might be important in more than 1 principle component.\n", "\n", "Remember that having these un-rotated variables is not the point of PCA, the point is to rotate the data to a more variance-centric view, and to remove what is not needed by selecting only a few number of variables of all the available variables."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Mutual Information (MI)\n", "\n", "PCA is a brilliant way to remove correlated variables but it only notices linear relationships between variables.\n", "\n", "Mutual Information (MI) on the other hand finds **all** relationships.\n", "\n", "Mutual information is a method of finding relationships between variables. \n", "\n", "MI measures how much certainty can be gained about Variable 2 by knowing Variable 1.\n", "\n", "The amount of 'uncertainty' is measured by entropy. \n", "Entropy is a fundamental measure used in Information Theory, which is the average  amount of information or certainty in a variable's possible outcomes. \n", "\n", "Entropy is the expected value of the information content. Shannon Information or the \"level of surprise\" of a variable, signifies how surprising a variable or message is on average.\n", "\n", "Information content has units of bits or *shannons*. If an event is unlikely, then it is more surprising and informative when it does happen. So, the value of information content decreases as the probability of its occurrence increases.\n", "\n", "Information, $I$, is defined as \n", "$I(E)=-log_2(P(E))$ \n", "where $E$ is an event, $P(E)$ is the probability of Event happening, and $log_2$ is logarithm base 2.\n", "https://en.wikipedia.org/wiki/Entropy_(information_theory)\n", "\n", "Value for MI can be in the range of zero to infinity.\n", "$$I(X;Y) = D_{KL}(P_{X,Y}|| P_X x P_Y)$$\n", "\n", "Where $D_{KL}$ is the Kullback-Liebler divergence, $P_{X,Y}$ is the joint distributions and $P_X$ and $P_Y$ are the marginal distributions.\n", "\n", "A marginal distribution https://en.wikipedia.org/wiki/Marginal_distribution"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Q11**. \n", "Calculate the mutual information score between variables df[\"RM\"] and df[\"CRIM\"].\n", "\n", "_Hint: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html_"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Q12**. Get the mutual information scores for all variables in `df` vs the target, \"MEDV\" and print them out. \n", "\n", "Hint: for this, you may use the mutual_info_regression function"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Your code here...\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Interpretation: \n", "LSTAT has the most mutual information with MEDV, the target. \n", "\n", "Credit: Kaggle Mutual Information"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Additional Material: Genetic Algorithms: evolving features\n", "\n", "Genetic Algorithms can also be an effective and cool tool to select features.\n", "\n", "You may find out more about them here: https://pypi.org/project/sklearn-genetic/#description\n", "\n", "### Extra tool you could read about if you like:\n", "A tool you can use for generating new features from original ones:\n", "\n", "[tsfresh](https://tsfresh.readthedocs.io/en/latest/text/quick_start.html)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Conclusions\n", "\n", "During the lesson, we have seen some examples of feature selection before the ML models are trained.\n", "\n", "Instead, feature engineering should be an iterative process.\n", "\n", "So, the features are improved after the ML model is trained, then do a new ML model. Iterate!\n", "\n", "Also, don't forget to tune hyperparameters of the ML model.\n", "\n", "In this practical we have seen how feature selection can be done in 3 different ways:\n", "\n", "+ Mutual Information\n", "+ Conditional entropy\n", "+ Information gain\n"]}]}