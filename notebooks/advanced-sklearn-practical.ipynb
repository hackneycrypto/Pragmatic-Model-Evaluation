{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modular pipelines with scikit-learn\n",
    "\n",
    "In this notebook you will practice how to make your analyses modular, using `sklearn` tools. You will see how this structure can help build more sophisticated setups.\n",
    "\n",
    "We will work on a classification task, using a credit risk dataset from Taiwan, with the goal of predicting the risk of credit default.\n",
    "\n",
    "[Dataset reference](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, FunctionTransformer, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, ShuffleSplit\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# For custom transformers\n",
    "from sklearn.base import TransformerMixin, BaseEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Load the dataset from `data/default.xls`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlrd\n",
      "  Downloading xlrd-2.0.2-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Downloading xlrd-2.0.2-py2.py3-none-any.whl (96 kB)\n",
      "Installing collected packages: xlrd\n",
      "Successfully installed xlrd-2.0.2\n"
     ]
    }
   ],
   "source": [
    "# Install xlrd for reading .xls files\n",
    "%pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1      20000    2          2         1   24      2      2     -1     -1   \n",
       "1   2     120000    2          2         2   26     -1      2      0      0   \n",
       "2   3      90000    2          2         2   34      0      0      0      0   \n",
       "3   4      50000    2          2         1   37      0      0      0      0   \n",
       "4   5      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...          0          0          0         0       689         0   \n",
       "1  ...       3272       3455       3261         0      1000      1000   \n",
       "2  ...      14331      14948      15549      1518      1500      1000   \n",
       "3  ...      28314      28959      29547      2000      2019      1200   \n",
       "4  ...      20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "0         0         0         0                           1  \n",
       "1      1000         0      2000                           1  \n",
       "2      1000      1000      5000                           0  \n",
       "3      1100      1069      1000                           0  \n",
       "4      9000       689       679                           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"../data/default.xls\", skiprows=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Create data matrices\n",
    "\n",
    "- Create a feature matrix `X` and a target vector `y` from `data`. \n",
    "    * The target to be predicted is given in the last column. \n",
    "    * The feature matrix should consist of the rest of the columns (except for `'ID'`).\n",
    "- Create train/test splits with `train_test_split()`, specifying that the data should be stratified by `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here...\n",
    "X = data.drop(columns=[\"ID\",\"default payment next month\"]) # could also do X = data.iloc[:,1:-1]\n",
    "y = data[\"default payment next month\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets have a look at the unique values in the target to check whether the dataset is imbalanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y: [0 1]\n",
      "Number of 1s in y: 6636/30000\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values in y: {}\".format(np.unique(y)))\n",
    "print(\"Number of 1s in y: {}/{}\".format(sum(y), len(y))) # data is imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24000, 23), (6000, 23))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Fit model and classify\n",
    "Let's use `.fit()` and find the score of a Decision Tree classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.7186666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82      4673\n",
      "           1       0.37      0.39      0.38      1327\n",
      "\n",
      "    accuracy                           0.72      6000\n",
      "   macro avg       0.60      0.60      0.60      6000\n",
      "weighted avg       0.72      0.72      0.72      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg = DecisionTreeClassifier()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "print(\"Score: \", logreg.score(X_test, y_test))\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Classification Report\n",
    "\n",
    "The classification report provides several key metrics for evaluating your model's performance:\n",
    "\n",
    "**Per-Class Metrics:**\n",
    "- **Precision**: Of all predictions for this class, what percentage were correct?\n",
    "  - Class 0 (no default): 83% - when the model predicts \"no default\", it's correct 83% of the time\n",
    "  - Class 1 (default): 37% - when it predicts \"default\", it's only correct 37% of the time\n",
    "  \n",
    "- **Recall (Sensitivity)**: Of all actual instances of this class, what percentage did we catch?\n",
    "  - Class 0: 81% - we correctly identify 81% of actual non-defaulters\n",
    "  - Class 1: 39% - we only catch 39% of actual defaulters (missing 61%!)\n",
    "  \n",
    "- **F1-Score**: Harmonic mean of precision and recall (balances both metrics)\n",
    "  - Class 0: 0.82 (good performance)\n",
    "  - Class 1: 0.38 (poor performance)\n",
    "  \n",
    "- **Support**: Number of actual instances in each class in the test set\n",
    "  - 4,673 non-defaulters vs 1,327 defaulters (confirming the imbalance)\n",
    "\n",
    "**Overall Metrics:**\n",
    "- **Accuracy (0.72)**: Overall, 72% of predictions are correct - but this is misleading with imbalanced data!\n",
    "- **Macro avg**: Simple average across classes (treats both equally, regardless of imbalance)\n",
    "- **Weighted avg**: Average weighted by support (gives more weight to the majority class)\n",
    "\n",
    "**⚠️ Key Insight for Imbalanced Data:**\n",
    "The model performs well on the majority class (0) but poorly on the minority class (1 - defaults). This is problematic because in credit risk, **missing a defaulter (39% recall) is often more costly than falsely flagging a non-defaulter**. The high overall accuracy (72%) masks this poor performance on the critical minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Scikit-learn pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now use the pipeline function of scikit-learn to arrive at the same results in three lines of code. Create a simple Decision Tree pipeline with `make_pipeline()` and test its predictions with `cross_val_score()`. Use a single split from now on, by setting `cv` to `ShuffleSplit(n_splits=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82      4673\n",
      "           1       0.37      0.39      0.38      1327\n",
      "\n",
      "    accuracy                           0.72      6000\n",
      "   macro avg       0.60      0.60      0.60      6000\n",
      "weighted avg       0.72      0.72      0.72      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here...\n",
    "pipeline = make_pipeline(DecisionTreeClassifier())\n",
    "split = ShuffleSplit(n_splits=1, random_state=42) # define a single split. If we have multiple splits, we need to aggregate the results\n",
    "cross_val_score(pipeline, X, y, cv=split)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our initial analysis overlooked several aspects that may hinder the performance of a predictive model. One is that the features should be transformed to an appropriate input type for a linear model. Here are the features we want to build:\n",
    "* `SEX`, `EDUCATION`, `MARRIAGE`: One-hot encoded features for categorical columns.\n",
    "* `PAY_...`, `BILL_AMT...`, `PAY_AMT...`: these features correspond to bills and payments made in the previous months. They're highly correlated so we will first scale them and then apply PCA.\n",
    "* `AGE`, `LIMIT_BAL`: these are numerical values. We will use `FeatureUnion` to create log and polynomial versions of them.\n",
    "* `Unpaid_by_user` / `avg_unpaid`: the ratio amount unpaid by the user divided by the average unpaid bill last month for all users. This should give us an idea of how bad at paying bills the user is.\n",
    "\n",
    "We'll go through each of them step-by-step, leveraging scikit-learn transformers.\n",
    "\n",
    "#### Column one-hot transformation\n",
    "\n",
    "We will create one-hot encoded features for the categorical columns `SEX`, `EDUCATION`, `MARRIAGE`. The standard process would be to create the new columns and add them to a new data matrix, and for each such preprocessing step we would have to create a new data set variation. Instead, we can create a transformation to be used in a pipeline.\n",
    "\n",
    "Create a one-hot transformer for the categorical variables, using `make_column_transformer()` and the `OneHotEncoder()`. The `ColumnTransformer` allows you to apply different transformations in parallel to different columns. Remember to set `remainder='passthrough'` to retain all other columns. \n",
    "\n",
    "Create a pipeline with the one-hot preprocessor and a Decision Tree. Test the model predictions with `cross_val_score()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82      4673\n",
      "           1       0.37      0.39      0.38      1327\n",
      "\n",
      "    accuracy                           0.72      6000\n",
      "   macro avg       0.60      0.60      0.60      6000\n",
      "weighted avg       0.72      0.72      0.72      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here...\n",
    "cat_cols = [\"SEX\", \"EDUCATION\", \"MARRIAGE\"]\n",
    "\n",
    "one_hot_processor = make_column_transformer(\n",
    "    (OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first'), cat_cols),\n",
    "    remainder='passthrough')\n",
    "\n",
    "pipeline = make_pipeline(one_hot_processor, DecisionTreeClassifier())\n",
    "cross_val_score(pipeline, X, y, cv=split) # cv=split means we use the same split as before\n",
    "\n",
    "print(classification_report(y_test, y_pred)) # Note that report is same as before because one-hot encoding doesnt make a big difference with decision trees as they can handle categorical variables inherently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the One-Hot Encoding Implementation\n",
    "\n",
    "**Step-by-step breakdown:**\n",
    "\n",
    "1. **Define categorical columns:**\n",
    "   ```python\n",
    "   cat_cols = [\"SEX\", \"EDUCATION\", \"MARRIAGE\"]\n",
    "   ```\n",
    "   These are the columns containing categorical data that need encoding.\n",
    "\n",
    "2. **Create the ColumnTransformer:**\n",
    "   ```python\n",
    "   one_hot_processor = make_column_transformer(\n",
    "       (OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first'), cat_cols),\n",
    "       remainder='passthrough')\n",
    "   ```\n",
    "   \n",
    "   **OneHotEncoder parameters:**\n",
    "   - `sparse_output=False`: Returns a dense array (regular numpy array) instead of sparse matrix - easier to work with\n",
    "   - `handle_unknown='ignore'`: If new categories appear in test/production data, ignore them gracefully (critical for deployment!)\n",
    "   - `drop='first'`: Drops first category to avoid multicollinearity (the \"dummy variable trap\")\n",
    "   \n",
    "   **ColumnTransformer behavior:**\n",
    "   - Applies `OneHotEncoder` to `SEX`, `EDUCATION`, `MARRIAGE`\n",
    "   - `remainder='passthrough'`: Keeps all other columns unchanged (AGE, LIMIT_BAL, payment history, etc.)\n",
    "\n",
    "3. **Create pipeline:**\n",
    "   ```python\n",
    "   pipeline = make_pipeline(one_hot_processor, DecisionTreeClassifier())\n",
    "   ```\n",
    "   Chains preprocessing with modeling in a single object.\n",
    "\n",
    "4. **Evaluate with cross-validation:**\n",
    "   ```python\n",
    "   cross_val_score(pipeline, X, y, cv=split)\n",
    "   ```\n",
    "   Tests the complete pipeline (preprocessing + model) on the data.\n",
    "\n",
    "**Example transformation:**\n",
    "- **Before:** `SEX=1` (male), `SEX=2` (female)\n",
    "- **After (with drop='first'):** Creates `SEX_2` column → 1 if female, 0 if male\n",
    "- The first category (male) becomes the reference baseline\n",
    "\n",
    "**Why pipelines?**\n",
    "✅ Prevents data leakage (fit only on training data in each fold)\n",
    "✅ Makes code cleaner and more maintainable\n",
    "✅ Easier deployment (preprocessing + model in one object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Reducing correlated features with Scaler and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will scale the payment columns and reduce their dimensionality with PCA.\n",
    "\n",
    "Create a sequential transformer that first scales the data and then applies PCA.\n",
    "\n",
    "Choose a sensible value for `n_components`, which sets the number of components to keep (note that `n_components` can be specified as a `float` saying what proportion of variance is retained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here...\n",
    "# Create a pipeline that scales then applies PCA\n",
    "# Keep 80% of variance (or use a specific number like n_components=5)\n",
    "payment_transformer = make_pipeline(\n",
    "    MinMaxScaler(), # is a safer scaling method that scales features to a range between 0 and 1. Default is StandardScaler which standardizes to mean 0 and variance 1\n",
    "    PCA(n_components=.8) # result is to get enough components to explain 80% of variance. 80% is a good rule of thumb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Great, let's keep this transformer for later when we'll group all the pieces together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Adding handcrafted features with `FeatureUnion`\n",
    "\n",
    "`FeatureUnion` can be applied on the data to add features (for example: interaction features or transformed features). In this dataset we want to create new features by combining and transforming `AGE` and `LIMIT_BAL`.\n",
    "\n",
    "Create a transformer that adds the log features with `FunctionTransformer()`, and a polynomial pairwise transformation (check the `PolynomialFeatures` documentation). For the latter transformer, specify `include_bias=False` and `interaction_only=True`.\n",
    "\n",
    "Test the transformation by calling `.fit_transform()` on `X_train`, columns `AGE` and `LIMIT_BAL`. How many new columns does this transformation create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here...\n",
    "numerical_transformer = FeatureUnion([\n",
    "    ('log', FunctionTransformer(np.log)), # apply log transformation\n",
    "    ('polynomial', PolynomialFeatures(2, include_bias=False, # apply polynomial features transformation\n",
    "    interaction_only=True)) # e.g., AGE^2, AGE*LIMIT_BAL, LIMIT_BAL^2\n",
    "])\n",
    "\n",
    "numerical_transformer.fit_transform(X_train[[\"AGE\", \"LIMIT_BAL\"]]).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the FeatureUnion and Numerical Transformer\n",
    "\n",
    "**What is FeatureUnion?**\n",
    "\n",
    "`FeatureUnion` combines multiple transformers in **parallel** (horizontally stacking features), unlike `Pipeline` which chains them sequentially.\n",
    "\n",
    "**Breaking down the code:**\n",
    "\n",
    "1. **`FunctionTransformer(np.log)`**:\n",
    "   - Applies logarithmic transformation to both `AGE` and `LIMIT_BAL`\n",
    "   - Creates 2 new features: `log(AGE)` and `log(LIMIT_BAL)`\n",
    "   - Why? Log transforms can help handle skewed distributions and reduce the effect of outliers\n",
    "\n",
    "2. **`PolynomialFeatures(2, include_bias=False, interaction_only=True)`**:\n",
    "   - `degree=2`: Considers up to 2nd degree polynomial features\n",
    "   - `include_bias=False`: Doesn't add a constant column (bias term)\n",
    "   - `interaction_only=True`: **Only creates the interaction term** (`AGE × LIMIT_BAL`), not squared terms\n",
    "   - Creates 3 features: `AGE`, `LIMIT_BAL`, `AGE × LIMIT_BAL`\n",
    "\n",
    "**Total features created: 5**\n",
    "- From log transformer: 2 features (log_AGE, log_LIMIT_BAL)\n",
    "- From polynomial transformer: 3 features (AGE, LIMIT_BAL, AGE×LIMIT_BAL)\n",
    "- These 5 features are concatenated horizontally (side-by-side)\n",
    "\n",
    "**Important Note:** The comment in the code saying \"e.g., AGE^2, AGE*LIMIT_BAL, LIMIT_BAL^2\" is **misleading**! With `interaction_only=True`, you **don't** get squared terms (AGE², LIMIT_BAL²). You only get the original features plus their interaction.\n",
    "\n",
    "**Why use FeatureUnion?**\n",
    "- Captures different aspects of the data (linear, logarithmic, and interaction effects)\n",
    "- Enriches feature space for the model to learn from\n",
    "- Allows the model to discover non-linear relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that if you have two features `a` and `b`, then `PolynomialFeatures(2, include_bias=False, interaction_only=True)` includes features `a`, `b` and `a*b`. If you remove `interaction_only`, it would include `a`, `b`, `a*b`, `a**2`, `b**2`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Custom Transformer\n",
    "\n",
    "We'll create a custom transformer that indicates if a user is above or below average for the unpaid bill last month: `unpaid_by_user > avg_unpaid`.\n",
    "\n",
    "For this we need to memorise the average amount unpaid by all users in the `.fit()` step, and then apply the transformation for any user in the `.transform()` step.\n",
    "\n",
    "We've defined the template for the class you need to implement below. Note that it extends `TransformerMixin` and `BaseEstimator`; that's required to define a new `Transformer` in the right format for `sklearn`.\n",
    "\n",
    "Follow the instructions below. You can run the code in the next cell to test your implementation.\n",
    "\n",
    "1. Implement the `.fit()` method:\n",
    "  * This method takes `X` and `y` (`y` is optional). \n",
    "  * It needs to compute `avg_unpaid_6`, `avg_unpaid_5`, and `avg_unpaid_4` - the amount unpaid on average for all users in the given training set for month 4, 5, 6. For example `avg_unpaid_6` is the mean of `BILL_AMT6` - `PAY_AMT6`.\n",
    "  * To be compatible with other `sklearn` tools, `.fit()` needs to return `self`.\n",
    "\n",
    "\n",
    "2. Implement the `.transform()` method:\n",
    "  * This method takes `X` as input and needs to return a new DataFrame with columns `Unpaid_ratio_6`, `Unpaid_ratio_5`, and `Unpaid_ratio_4`, containing the same number of rows as `X`.\n",
    "  * Each value indicates if the amount unpaid by user (`BILL_AMT` - `PAY_AMT`) is higher than the average amount saved in `.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class UnpaidTransformer(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Your code here...\n",
    "        self.avg_unpaid_6 = (X[\"BILL_AMT6\"] - X[\"PAY_AMT6\"]).mean()\n",
    "        self.avg_unpaid_5 = (X[\"BILL_AMT5\"] - X[\"PAY_AMT5\"]).mean()\n",
    "        self.avg_unpaid_4 = (X[\"BILL_AMT4\"] - X[\"PAY_AMT4\"]).mean()\n",
    "        \n",
    "        return self\n",
    "        pass # --- IGNORE ---\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Your code here...\n",
    "        X_6 = (X[\"BILL_AMT6\"] - X[\"PAY_AMT6\"]) > self.avg_unpaid_6\n",
    "        X_5 = (X[\"BILL_AMT5\"] - X[\"PAY_AMT5\"]) > self.avg_unpaid_5\n",
    "        X_4 = (X[\"BILL_AMT4\"] - X[\"PAY_AMT4\"]) > self.avg_unpaid_4\n",
    "\n",
    "        X_new = pd.DataFrame({\"Unpaid_ratio_6\": X_6, \"Unpaid_ratio_5\": X_5, \"Unpaid_ratio_4\": X_4})\n",
    "        return X_new # New dataFrame with the new features\n",
    "        pass # --- IGNORE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33446.074375\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unpaid_ratio_6</th>\n",
       "      <th>Unpaid_ratio_5</th>\n",
       "      <th>Unpaid_ratio_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22788</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29006</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16950</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22280</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11346</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unpaid_ratio_6  Unpaid_ratio_5  Unpaid_ratio_4\n",
       "22788            True            True            True\n",
       "29006           False           False           False\n",
       "16950           False           False           False\n",
       "22280            True            True            True\n",
       "11346           False           False           False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the transformer\n",
    "unpaid_transfomer = UnpaidTransformer()\n",
    "\n",
    "# Test .fit()\n",
    "unpaid_transfomer.fit(X_train)\n",
    "print(unpaid_transfomer.avg_unpaid_6)\n",
    "\n",
    "# Test .transform()\n",
    "unpaid_transfomer.transform(X_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Build a joint preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we have all the transformers we need, we can create a single object using `ColumnTransformer`. First we need to define a list of columns that will be processed separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "payment_cols = [col for col in X_train.columns if col.startswith(\"PAY_\") or col.startswith(\"BILL_\")]\n",
    "\n",
    "num_cols = [\"AGE\", \"LIMIT_BAL\"]\n",
    "\n",
    "# Features needed to compute the unpaid features:\n",
    "unpaid_cols = [\"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\"]\n",
    "cat_cols = [\"SEX\", \"EDUCATION\", \"MARRIAGE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's create a `ColumnTransfomer` object, which takes a list of tuples as input. Each tuple needs to have the following format: (`name`, `transformer`, `columns`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here...\n",
    "preprocessor = ColumnTransformer([(\"payments\", payment_transformer, payment_cols), \n",
    "                                  (\"numerical\", numerical_transformer, num_cols),\n",
    "                                  (\"unpaid\", unpaid_transfomer, unpaid_cols),\n",
    "                                  (\"categorical\", OneHotEncoder(), cat_cols)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we have a preprocessor object that we can reuse for any new data. The next step is to create a predictive model with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizing models\n",
    "\n",
    "In order to evaluate different predictive algorithms, we can create several pipeline objects, each combining our preprocessor together with a different algorithm. \n",
    "\n",
    "Create new pipeline objects (`dtc` and `rfc`) with our joint preprocessor for the Decision Tree and the Random Forest classifiers. Test them with `cross_val_score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72266667]\n",
      "[0.805]\n",
      "[0.805]\n"
     ]
    }
   ],
   "source": [
    "# Your code here...\n",
    "dtc = make_pipeline(preprocessor, DecisionTreeClassifier())\n",
    "print(cross_val_score(dtc,X,y,cv=split))\n",
    "\n",
    "rfc = make_pipeline(preprocessor, RandomForestClassifier()) # better performance than decision tree as it is an ensemble method\n",
    "print(cross_val_score(rfc,X,y,cv=split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Using grid search with a pipeline\n",
    "\n",
    "One major advantage of using a pipeline is that now we have both the processing **and** predictive modeling in one object, which can be trained and optimized jointly. It means that on every fold it is trained on, we will refit the model and also the transformations, which ensures we do not have data leakage and makes the evaluation more accurate. \n",
    "\n",
    "When tuning our model we can tune the hyperparameters of the model, and also some processing parameters, such as the number of PCA components. This would be much harder with a regular sequential preprocessing-then-training scenario.\n",
    "\n",
    "Below we use grid search to tune the `max_depth` and `min_samples_split` of the Random Forest model, together with the `n_components` in PCA.\n",
    "\n",
    "Note: to refer to a parameter, you need to use the name of the step, followed by `__`, then the parameter name. Since the PCA model is deep inside our pipeline, the name is a bit more complicated. Using `.named_steps` allows us to see what's inside our pipeline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columntransformer': ColumnTransformer(transformers=[('payments',\n",
       "                                  Pipeline(steps=[('minmaxscaler',\n",
       "                                                   MinMaxScaler()),\n",
       "                                                  ('pca',\n",
       "                                                   PCA(n_components=0.8))]),\n",
       "                                  ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5',\n",
       "                                   'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n",
       "                                   'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5',\n",
       "                                   'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2',\n",
       "                                   'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5',\n",
       "                                   'PAY_AMT6']),\n",
       "                                 ('numerical',\n",
       "                                  FeatureUnion(transformer_list=[('log',\n",
       "                                                                  FunctionTransformer(func=<ufunc 'log'>)),\n",
       "                                                                 ('polynomial',\n",
       "                                                                  PolynomialFeatures(include_bias=False,\n",
       "                                                                                     interaction_only=True))]),\n",
       "                                  ['AGE', 'LIMIT_BAL']),\n",
       "                                 ('unpaid', UnpaidTransformer(),\n",
       "                                  ['PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
       "                                   'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']),\n",
       "                                 ('categorical', OneHotEncoder(),\n",
       "                                  ['SEX', 'EDUCATION', 'MARRIAGE'])]),\n",
       " 'randomforestclassifier': RandomForestClassifier()}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.named_steps # view the steps in the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here PCA is in `columntransformer`, then `payments` (the Pipeline), then `pca` (one can use `.get_params()` to make sure we got the name of the parameter right). \n",
    "\n",
    "Use `GridSearchCV()` to find the parameters with the best accuracy. Print these parameters and the corresponding score, with `.best_params_` and `.best_score_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'columntransformer__payments__pca__n_components': 0.98, 'randomforestclassifier__max_depth': 5, 'randomforestclassifier__min_samples_split': 2}\n",
      "0.81125\n"
     ]
    }
   ],
   "source": [
    "# Your code here...\n",
    "params = {'columntransformer__payments__pca__n_components': [0.5, 0.98], # __ to access nested parameters in the pipeline. Travel from outside to inside\n",
    "          'randomforestclassifier__max_depth': [5, 30],\n",
    "          'randomforestclassifier__min_samples_split': [2, 5]}\n",
    "\n",
    "gcv = GridSearchCV(rfc, param_grid=params, n_jobs=-1, cv=split)\n",
    "gcv.fit(X_train, y_train)\n",
    "\n",
    "print(gcv.best_params_)\n",
    "print(gcv.best_score_) # score is a minor improvement over previous score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Recover and fit the model with the best hyper-parameters\n",
    "\n",
    "You can recover the best hyper-parameters of the grid-search with `.set_params()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here...\n",
    "rfc.set_params(**gcv.best_params_); # ** unpacks the dictionary of best params\n",
    "rfc.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Saving pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now save the best pre-trained model to be used later, using the `pickle` library. Write your model to `\"data/rf_model.pickle\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here...\n",
    "with open(\"../data/rf_model.pickle\", \"wb\") as f:\n",
    "    pickle.dump(rfc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now load your model and print its score again, to check that you have successfully saved it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80733333])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/rf_model.pickle\", \"rb\") as f:\n",
    "    rfc_pickle = pickle.load(f)\n",
    "cross_val_score(rfc_pickle, X, y, cv=split) # results are variable because of random shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pickle is really useful to save trained models, but keep in mind that it does not save dependencies. To be able to load your model, you need to import the code it relies on: external libraries and our own class definitions. Because of this, it is common to get errors when trying to load a model trained with an old version of a library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Reproducible experiments\n",
    "\n",
    "To make sure your model was not just lucky to get good results for a given training session, it is important to know how to make reproducible experiments. Apart from having the same code and data of course, to be reproducible, one needs to have same random number generator state. \n",
    "\n",
    "Run three instances of training your pipeline and testing it with `cross_val_score()`. Before two of them set the random seed to `1234`, with `np.random.seed()`. Do the results match exactly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81966667])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here...\n",
    "np.random.seed(); \n",
    "rfc1 = make_pipeline(preprocessor, RandomForestClassifier()) \n",
    "cross_val_score(rfc1, X, y, cv=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.813])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here...\n",
    "np.random.seed(1234)\n",
    "rfc2 = make_pipeline(preprocessor, RandomForestClassifier()) \n",
    "cross_val_score(rfc2, X, y, cv=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.813])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here...\n",
    "np.random.seed(1234)\n",
    "rfc3 = make_pipeline(preprocessor, RandomForestClassifier()) \n",
    "cross_val_score(rfc3, X, y, cv=split) # same results as rfc2 because of same random seed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
